{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acab722c-1d44-4d00-8625-34c3cb90c569",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# !pip install -q datasets\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mtorch\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "# !pip install -q datasets\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b602f310-f1af-40a1-9bbc-b2f8076b0210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61de8c2b-12e2-4e3e-8faf-a574dc325fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/s0001666/Working_dir/my_repos/GPT_my/gpt_impl/src/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d11d35ff-4ec3-42c8-8b34-741e1f41cdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gptmini.data.dataloader import MyGPTDataset, gpt_collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a1d0930-05bf-494d-811a-a47510eaf938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1801350 3760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s0001666/.local/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"Salesforce/wikitext\", \"wikitext-103-raw-v1\")\n",
    "train = ds[\"train\"]\n",
    "val   = ds[\"validation\"]\n",
    "\n",
    "print(len(train), len(val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c37736c-7658-42c6-b5d3-650bf05602ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in Janu\n"
     ]
    }
   ],
   "source": [
    "print(train[3][\"text\"][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64a76fc6-3786-4e47-b314-c203ebb769c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ''}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db84c981-cc2a-4331-ada3-60f550cd9b00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ' Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" . \\n'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebba600f-b1f4-435a-a086-32ece8472bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-26 14:54:19.698908: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-26 14:54:19.723622: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2026-01-26 14:54:19.723668: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2026-01-26 14:54:19.723689: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2026-01-26 14:54:19.729295: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-26 14:54:20.294427: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14402, 3275, 326, 314, 765, 284, 37773, 13] Test message that I want to encode.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", use_fast=True)\n",
    "\n",
    "# GPT-2 tokenizer has no pad token by default; if you ever need padding, do this:\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# quick check\n",
    "s = \"Test message that I want to encode.\"\n",
    "ids = tokenizer.encode(s)\n",
    "print(ids, tokenizer.decode(ids))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377c6ebc-402c-4cb5-a295-f1b19253854f",
   "metadata": {},
   "source": [
    "### Make streams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35b69a0d-657b-4fe6-8b00-d8cdb0e26a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "eos = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "faba61d0-f7a4-4eec-a5ef-600626bf1876",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = [t for t in ds[\"train\"][\"text\"] if t.strip()]\n",
    "val_texts   = [t for t in ds[\"validation\"][\"text\"] if t.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0dd0403-8cfa-48f5-95ff-76cba0829699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1165029, 2461)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_texts), len(val_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af83dfcf-99b5-4e68-9e36-5a18c4efba0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "842c44e5-85ab-475a-8ad0-43361df286c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_stream(texts, batch_size=1024):\n",
    "    stream = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        enc = tokenizer(batch, add_special_tokens=False).input_ids  # list[list[int]]\n",
    "        for ids in enc:\n",
    "            stream.extend(ids)\n",
    "            stream.append(eos)\n",
    "    return torch.tensor(stream, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4ecc659-342b-4bcf-a59a-ee91c04efcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_stream   = build_stream(val_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "114cf93b-3e07-4fb1-8c12-d1db6ba2cdeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1063 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "train_stream = build_stream(train_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db7c67f5-2f3a-45ac-a09f-81d87e62656c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([119085169]) torch.Size([249750])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(train_stream.shape, val_stream.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29f7a941-323f-4921-842c-20bb30da7114",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyGPTDataset(train_stream, 256)\n",
    "eval_dataset = MyGPTDataset(val_stream, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eba928ab-c3a2-4949-9db3-e7b6a7ff13f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=16, shuffle=True, drop_last=True,\n",
    "    num_workers=2, pin_memory=True, collate_fn=gpt_collate_fn\n",
    ")\n",
    "\n",
    "eval_loader = DataLoader(\n",
    "    eval_dataset, batch_size=32, shuffle=False, drop_last=False,\n",
    "    num_workers=2, pin_memory=True, collate_fn=gpt_collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45a04b5e-85d8-4df6-9eba-81bbf3d17115",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "x = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b56bc973-c316-44bf-8340-f884ad1a03c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 257])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f04126b-c0ef-497a-82cb-d2cbb5d9dc48",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49fb1577-d4cf-462b-8cb0-4e25863c8485",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gptmini.model.gpt import GPTmini\n",
    "\n",
    "from gptmini.trainer import train_step, eval_step, TransLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ccb9301e-c56d-456e-9b48-8f4b4fd387eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters \n",
    "vocab_len = len(tokenizer)\n",
    "max_len = 256\n",
    "input_dim = 256\n",
    "n_layers = 4\n",
    "n_heads = 4\n",
    "feature_dim = 256\n",
    "ffn_dim = input_dim * 4\n",
    "ffn_dropout = 0.1\n",
    "attn_dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "991a6031-c56a-47e4-9c7c-e7b0ca9ef507",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTmini(\n",
    "    max_len,\n",
    "        vocab_len,\n",
    "        input_dim,\n",
    "        n_layers,\n",
    "        n_heads,\n",
    "        feature_dim,\n",
    "        ffn_dim,\n",
    "        ffn_dropout=0.0,\n",
    "        attn_dropout=0.0,\n",
    "        emb_type='rotary',           \n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330d88f2-1ec9-4971-8122-f9cd1e11ba3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss_fn = TransLoss(label_smoothing=0.1)\n",
    "optim = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    weight_decay=0.1,\n",
    "    lr=3e-4,                \n",
    "    betas=(0.9, 0.95),\n",
    "    eps=1e-8,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "939192e0-27b1-4c9b-9d16-8bc4dfabc96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = input_dim\n",
    "warmup_steps = 1000\n",
    "\n",
    "def noam_lr(step: int):\n",
    "    step = max(step, 1)\n",
    "    return (d_model ** -0.5) * min(step ** -0.5, step * (warmup_steps ** -1.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6bb23bd-2ef4-4559-a346-c4ffcc118fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "#     optim, lr_lambda=lambda step: noam_lr(step)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c5cc06-f7ae-43f3-8119-9c3a1cc21da4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad1fdfdb-df2d-45d9-a8a8-27560caaaeff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings/source      total=0  trainable=0\n",
      "embeddings/target      total=0  trainable=0\n",
      "positional_encoding    total=0  trainable=0\n",
      "lm_head                total=0  trainable=0\n",
      "encoder/attn           total=0  trainable=0\n",
      "encoder/ffn            total=0  trainable=0\n",
      "encoder/norms          total=0  trainable=0\n",
      "decoder/self_attn      total=0  trainable=0\n",
      "decoder/cross_attn     total=0  trainable=0\n",
      "decoder/ffn            total=0  trainable=0\n",
      "decoder/norms          total=0  trainable=0\n",
      "other                  total=29,204,305  trainable=29,204,305\n",
      "\n",
      "Grand total: 29204305\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def count_params(p_iter):\n",
    "    return sum(p.numel() for p in p_iter)\n",
    "\n",
    "def group_param_counts(model):\n",
    "    \"\"\"\n",
    "    Groups params by Transformer blocks (encoder/decoder FFN/attn/norms + embeddings/head/PE).\n",
    "    Adjust rules if your attribute names differ.\n",
    "    \"\"\"\n",
    "    rules = OrderedDict({\n",
    "        # top-level\n",
    "        \"embeddings/source\":      {\"all\": [\"source_embedding\"]},\n",
    "        \"embeddings/target\":      {\"all\": [\"target_embedding\"]},\n",
    "        \"positional_encoding\":    {\"all\": [\"PE.\"]},          # buffer-only usually => 0 trainable\n",
    "        \"lm_head\":                {\"all\": [\"lm_head\"]},\n",
    "\n",
    "        # encoder\n",
    "        \"encoder/attn\":           {\"all\": [\"encoder.\"], \"any\": [\".mha.\"]},\n",
    "        \"encoder/ffn\":            {\"all\": [\"encoder.\"], \"any\": [\".ffn.\"]},\n",
    "        \"encoder/norms\":          {\"all\": [\"encoder.\"], \"any\": [\"norm\"]},\n",
    "\n",
    "        # decoder\n",
    "        \"decoder/self_attn\":      {\"all\": [\"decoder.\"], \"any\": [\".sa.\"]},\n",
    "        \"decoder/cross_attn\":     {\"all\": [\"decoder.\"], \"any\": [\".ca.\"]},\n",
    "        \"decoder/ffn\":            {\"all\": [\"decoder.\"], \"any\": [\".ffn.\"]},\n",
    "        \"decoder/norms\":          {\"all\": [\"decoder.\"], \"any\": [\"norm\"]},\n",
    "    })\n",
    "\n",
    "    # init buckets\n",
    "    buckets = OrderedDict((k, {\"total\": 0, \"trainable\": 0}) for k in rules.keys())\n",
    "    buckets[\"other\"] = {\"total\": 0, \"trainable\": 0}\n",
    "\n",
    "    def match(name, rule):\n",
    "        if \"all\" in rule and not all(s in name for s in rule[\"all\"]):\n",
    "            return False\n",
    "        if \"any\" in rule and not any(s in name for s in rule[\"any\"]):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    for name, p in model.named_parameters():\n",
    "        placed = False\n",
    "        for gname, rule in rules.items():\n",
    "            if match(name, rule):\n",
    "                buckets[gname][\"total\"] += p.numel()\n",
    "                if p.requires_grad:\n",
    "                    buckets[gname][\"trainable\"] += p.numel()\n",
    "                placed = True\n",
    "                break\n",
    "        if not placed:\n",
    "            buckets[\"other\"][\"total\"] += p.numel()\n",
    "            if p.requires_grad:\n",
    "                buckets[\"other\"][\"trainable\"] += p.numel()\n",
    "\n",
    "    return buckets\n",
    "\n",
    "# ---- usage ----\n",
    "buckets = group_param_counts(model)\n",
    "for k, v in buckets.items():\n",
    "    print(f\"{k:22s} total={v['total']:,}  trainable={v['trainable']:,}\")\n",
    "\n",
    "print(\"\\nGrand total:\", sum(v[\"total\"] for v in buckets.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "22497a73-1b2a-43ec-868a-26e473d9b433",
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_idx = -1\n",
    "num_epochs = 10\n",
    "log_every = int(len(train_loader) / 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf8ef1e-e631-4ec2-8bef-568626bc33d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cosine_warmup_scheduler(optim, warmup_steps, total_steps, min_lr_ratio=0.1):\n",
    "    def lr_lambda(step):\n",
    "        step = max(step, 1)\n",
    "        if step <= warmup_steps:\n",
    "            return step / warmup_steps\n",
    "        progress = (step - warmup_steps) / max(1, total_steps - warmup_steps)\n",
    "        cosine = 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "        return min_lr_ratio + (1.0 - min_lr_ratio) * cosine\n",
    "\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optim, lr_lambda)\n",
    "\n",
    "scheduler = make_cosine_warmup_scheduler(optim, warmup_steps=warmup_steps, total_steps=len(train_loader) * num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "92575b87-7ffe-44c8-b931-2284b13ce87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 step 2907 | train_loss 6.5666 | lr 0.0005796\n",
      "epoch 1 step 5814 | train_loss 6.1687 | lr 0.0004098\n",
      "epoch 1 step 8721 | train_loss 5.9820 | lr 0.0003346\n",
      "epoch 1 step 11628 | train_loss 5.8660 | lr 0.0002898\n",
      "epoch 1 step 14535 | train_loss 5.7853 | lr 0.0002592\n",
      "epoch 1 step 17442 | train_loss 5.7236 | lr 0.0002366\n",
      "epoch 1 step 20349 | train_loss 5.6753 | lr 0.0002191\n",
      "epoch 1 step 23256 | train_loss 5.6351 | lr 0.0002049\n",
      "epoch 1 step 26163 | train_loss 5.6007 | lr 0.0001932\n",
      "epoch 1 step 29070 | train_loss 5.5712 | lr 0.0001833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[32, 32, 4, 64]' is invalid for input of size 8192",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m eval_loader:\n\u001b[0;32m---> 24\u001b[0m     val_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43meval_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     26\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m val_sum \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mmax\u001b[39m(m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Working_dir/my_repos/GPT_my/gpt_impl/src/gptmini/trainer.py:57\u001b[0m, in \u001b[0;36meval_step\u001b[0;34m(model, batch, loss_fn, pad_id)\u001b[0m\n\u001b[1;32m     54\u001b[0m     src_ids \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     55\u001b[0m     src_ids \u001b[38;5;241m=\u001b[39m src_ids\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 57\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(logits, src_ids, pad_id)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Working_dir/my_repos/GPT_my/gpt_impl/src/gptmini/model/gpt.py:69\u001b[0m, in \u001b[0;36mGPTmini.forward\u001b[0;34m(self, x, attn_mask)\u001b[0m\n\u001b[1;32m     66\u001b[0m     pe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpe\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_layers:\n\u001b[0;32m---> 69\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[1;32m     72\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpt_head(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Working_dir/my_repos/GPT_my/gpt_impl/src/gptmini/model/layers.py:178\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x, pe, attn_mask)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, pe\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, attn_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 178\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmha\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_norm(x))\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Working_dir/my_repos/GPT_my/gpt_impl/src/gptmini/model/attention.py:79\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, data, context, pe, attn_mask)\u001b[0m\n\u001b[1;32m     76\u001b[0m H \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_heads\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# print(Q.shape, B, S1, H, self.head_dim)\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m Q \u001b[38;5;241m=\u001b[39m \u001b[43mQ\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mS1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     81\u001b[0m K \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39mview(B, S2, H, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     82\u001b[0m V \u001b[38;5;241m=\u001b[39m V\u001b[38;5;241m.\u001b[39mview(B, S2, H, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[32, 32, 4, 64]' is invalid for input of size 8192"
     ]
    }
   ],
   "source": [
    "global_step = 0\n",
    "t_losses = []\n",
    "v_losses = []\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # ---- train ----\n",
    "    running = 0.0\n",
    "    n = 0\n",
    "    for step, batch in enumerate(train_loader, start=1):\n",
    "        global_step += 1\n",
    "        loss = train_step(model, batch, optim, loss_fn, padding_idx, scheduler)\n",
    "        running += loss\n",
    "        n += 1\n",
    "\n",
    "        if step % log_every == 0:\n",
    "            print(f\"epoch {epoch} step {step} | train_loss {running/n:.4f} | lr {optim.param_groups[0]['lr']:.7f}\")\n",
    "\n",
    "    train_loss = running / max(n, 1)\n",
    "    t_losses.append(train_loss)\n",
    "    \n",
    "    # ---- valid ----\n",
    "    val_sum = 0.0\n",
    "    m = 0\n",
    "    for batch in eval_loader:\n",
    "        val_sum += eval_step(model, batch, loss_fn, padding_idx)\n",
    "        m += 1\n",
    "    val_loss = val_sum / max(m, 1)\n",
    "    v_losses.append(val_loss)\n",
    "    print(f\"\\nEPOCH {epoch} DONE | train_loss {train_loss:.4f} | val_loss {val_loss:.4f}\\n\")\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7e0052-06cd-41e6-97f5-6877b770d5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = torch.nn.Embedding(vocab_len, input_dim)\n",
    "l(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6943c483-1c68-445b-a23c-85075363dae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- sample generation ----\n",
    "if (tokenizer_de_en is not None) and (epoch % gen_every == 0):\n",
    "    model.eval()\n",
    "    src_ids, tgt_ids = next(iter(gpt_dataloader_eval))\n",
    "    src_ids = src_ids[:gen_k].to('cuda')\n",
    "    tgt_ids = tgt_ids[:gen_k]\n",
    "\n",
    "    pred_ids = model.generate(src_ids, bos_token=bos_token_id, eos_token=eos_token_id, strategy=\"beam\")\n",
    "\n",
    "    for i in range(min(gen_k, src_ids.size(0))):\n",
    "        src_txt = tokenizer_de_en.decode(src_ids[i].detach().cpu(), skip_special_tokens=False)\n",
    "        tgt_txt = tokenizer_de_en.decode(tgt_ids[i].detach().cpu(), skip_special_tokens=False)\n",
    "        pred_txt = tokenizer_de_en.decode(pred_ids[i].detach().cpu(), skip_special_tokens=False)\n",
    "        print(f\"[sample {i}] SRC : {src_txt}\")\n",
    "        print(f\"[sample {i}] TGT : {tgt_txt}\")\n",
    "        print(f\"[sample {i}] PRED: {pred_txt}\")\n",
    "        print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
